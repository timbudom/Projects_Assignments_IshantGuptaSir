{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4188c020",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56117190-3e48-4cc1-b0ab-f6a0ba22817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_from_disk\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42131d48",
   "metadata": {},
   "source": [
    "Loading the dataset & data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d662b4d8-2cd0-4362-9872-6af97acfd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ec61cc-93d6-4bac-8e72-07a5f629ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id  \\\n",
      "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
      "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
      "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
      "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
      "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
      "\n",
      "                                             article  \\\n",
      "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
      "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
      "2  A drunk driver who killed a young woman in a h...   \n",
      "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
      "4  Fleetwood are the only team still to have a 10...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Bishop John Folda, of North Dakota, is taking ...  \n",
      "1  Criminal complaint: Cop used his role to help ...  \n",
      "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
      "3  Nina dos Santos says Europe must be ready to a...  \n",
      "4  Fleetwood top of League One after 2-0 win at S...  \n",
      "                                         id  \\\n",
      "0  61df4979ac5fcc2b71be46ed6fe5a46ce7f071c3   \n",
      "1  21c0bd69b7e7df285c3d1b1cf56d4da925980a68   \n",
      "2  56f340189cd128194b2e7cb8c26bb900e3a848b4   \n",
      "3  00a665151b89a53e5a08a389df8334f4106494c2   \n",
      "4  9f6fbd3c497c4d28879bebebea220884f03eb41a   \n",
      "\n",
      "                                             article  \\\n",
      "0  Sally Forrest, an actress-dancer who graced th...   \n",
      "1  A middle-school teacher in China has inked hun...   \n",
      "2  A man convicted of killing the father and sist...   \n",
      "3  Avid rugby fan Prince Harry could barely watch...   \n",
      "4  A Triple M Radio producer has been inundated w...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Sally Forrest, an actress-dancer who graced th...  \n",
      "1  Works include pictures of Presidential Palace ...  \n",
      "2  Iftekhar Murtaza, 29, was convicted a year ago...  \n",
      "3  Prince Harry in attendance for England's crunc...  \n",
      "4  Nick Slater's colleagues uploaded a picture to...  \n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                             article  \\\n",
      "0  Ever noticed how plane seats appear to be gett...   \n",
      "1  A drunk teenage boy had to be rescued by secur...   \n",
      "2  Dougie Freedman is on the verge of agreeing a ...   \n",
      "3  Liverpool target Neto is also wanted by PSG an...   \n",
      "4  Bruce Jenner will break his silence in a two-h...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Experts question if  packed out planes are put...  \n",
      "1  Drunk teenage boy climbed into lion enclosure ...  \n",
      "2  Nottingham Forest are close to extending Dougi...  \n",
      "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
      "4  Tell-all interview with the reality TV star, 6...  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(val_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee693e43",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76967017-c4cc-4f9f-992c-578527fd25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "train_data['article'] = train_data['article'].apply(preprocess)\n",
    "train_data['highlights'] = train_data['highlights'].apply(preprocess)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(preprocess)\n",
    "val_data['highlights'] = val_data['highlights'].apply(preprocess)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(preprocess)\n",
    "test_data['highlights'] = test_data['highlights'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a95d2-2009-4c61-8961-84edecf949b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09ee66-310a-422b-8fac-0381a78f50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('cleaned_train_data.csv', index=False)\n",
    "val_data.to_csv('cleaned_val_data.csv', index=False)\n",
    "test_data.to_csv('cleaned_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a7a9e-d22c-48af-aeb6-274f8d02bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove common words that don't add much meaning (like \"the\", \"is\", \"in\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472fbe6-bcab-4df6-ae5e-04918aac472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "train_data['article'] = train_data['article'].apply(remove_stopwords)\n",
    "train_data['highlights'] = train_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(remove_stopwords)\n",
    "val_data['highlights'] = val_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(remove_stopwords)\n",
    "test_data['highlights'] = test_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "# Save data after removing stopwords\n",
    "train_data.to_csv('stopwords_removed_train_data.csv', index=False)\n",
    "val_data.to_csv('stopwords_removed_val_data.csv', index=False)\n",
    "test_data.to_csv('stopwords_removed_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7572f-584f-46e2-9444-213909e457ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use SpaCy for lemmatization to convert words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4c37b2-8247-4c47-a030-9b20f7ef8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c9e9eb-6986-4be5-a487-089c9c91a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy tqdm\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48cd0ac3-f788-4657-9771-3bc72a822942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 287113/287113 [3:35:38<00:00, 22.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 287113/287113 [14:04<00:00, 339.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 13368/13368 [08:01<00:00, 27.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 13368/13368 [00:41<00:00, 318.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 11490/11490 [07:00<00:00, 27.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 11490/11490 [00:35<00:00, 326.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_data.csv')\n",
    "val_data = pd.read_csv('cleaned_val_data.csv')\n",
    "test_data = pd.read_csv('cleaned_test_data.csv')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all datasets\n",
    "train_data['article'] = train_data['article'].apply(clean_text)\n",
    "train_data['highlights'] = train_data['highlights'].apply(clean_text)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(clean_text)\n",
    "val_data['highlights'] = val_data['highlights'].apply(clean_text)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(clean_text)\n",
    "test_data['highlights'] = test_data['highlights'].apply(clean_text)\n",
    "\n",
    "# Load the pre-trained SpaCy model globally\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function for lemmatization using nlp.pipe for batch processing\n",
    "def lemmatize_texts_with_progress(texts):\n",
    "    lemmatized_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=50, disable=['parser', 'ner']), total=len(texts)):\n",
    "        lemmatized_texts.append(' '.join([token.lemma_ for token in doc]))\n",
    "    return lemmatized_texts\n",
    "\n",
    "# Apply lemmatization with progress bar\n",
    "train_data['article'] = lemmatize_texts_with_progress(train_data['article'].tolist())\n",
    "train_data['highlights'] = lemmatize_texts_with_progress(train_data['highlights'].tolist())\n",
    "\n",
    "val_data['article'] = lemmatize_texts_with_progress(val_data['article'].tolist())\n",
    "val_data['highlights'] = lemmatize_texts_with_progress(val_data['highlights'].tolist())\n",
    "\n",
    "test_data['article'] = lemmatize_texts_with_progress(test_data['article'].tolist())\n",
    "test_data['highlights'] = lemmatize_texts_with_progress(test_data['highlights'].tolist())\n",
    "\n",
    "# Save lemmatized data\n",
    "train_data.to_csv('lemmatized_train_data.csv', index=False)\n",
    "val_data.to_csv('lemmatized_val_data.csv', index=False)\n",
    "test_data.to_csv('lemmatized_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63fecb34-a6f3-4df0-8b0a-361bb08c8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to replace all numbers in the text with a placeholder <NUM>\n",
    "def replace_numbers(text):\n",
    "    return re.sub(r'\\d+', '<NUM>', text)\n",
    "\n",
    "# Function to replace rare words in the text with a placeholder <RARE>\n",
    "# A word is considered rare if its frequency is below the specified threshold (default is 5)\n",
    "def remove_rare_words(text, freq_threshold=5):\n",
    "    words = text.split()  # Split the text into words\n",
    "    word_freq = Counter(words)  # Count the frequency of each word\n",
    "    # Identify words that occur less than the frequency threshold\n",
    "    rare_words = {word for word, freq in word_freq.items() if freq < freq_threshold}\n",
    "    # Replace rare words with <RARE>, keep other words unchanged\n",
    "    filtered_text = [word if word not in rare_words else '<RARE>' for word in words]\n",
    "    return ' '.join(filtered_text)  # Join the words back into a single string\n",
    "\n",
    "# Apply number replacement and rare word removal to the 'article' and 'highlights' columns of all datasets\n",
    "train_data['article'] = train_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "train_data['highlights'] = train_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "val_data['highlights'] = val_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "test_data['highlights'] = test_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "# Save the processed datasets with handled numbers and rare words\n",
    "train_data.to_csv('handled_train_data.csv', index=False)  # Save the training data\n",
    "val_data.to_csv('handled_val_data.csv', index=False)      # Save the validation data\n",
    "test_data.to_csv('handled_test_data.csv', index=False)    # Save the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbd06f1-63c3-4a2b-a698-c0a26066fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentence tokenization to the 'article' column\n",
    "# This splits the text of each article into a list of sentences\n",
    "train_data['sentences'] = train_data['article'].apply(sent_tokenize)\n",
    "val_data['sentences'] = val_data['article'].apply(sent_tokenize)\n",
    "test_data['sentences'] = test_data['article'].apply(sent_tokenize)\n",
    "\n",
    "# Save the datasets with the new 'sentences' column to CSV files\n",
    "train_data.to_csv('tokenized_train_data.csv', index=False)  # Save the training data\n",
    "val_data.to_csv('tokenized_val_data.csv', index=False)      # Save the validation data\n",
    "test_data.to_csv('tokenized_test_data.csv', index=False)    # Save the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7862220-f2cc-47d8-9e41-09722831bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea051fc",
   "metadata": {},
   "source": [
    "Preparing for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d784b8a7-9806-4b40-b821-348ef7f2a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65bd17dd6a94cd1ba80e5f4cd96992c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aa86433d55400288a55fab409e2c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614fb0719c164d4f8e3d032611db3c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Define a function to tokenize the 'article' and 'highlights' columns\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the 'article' text with a maximum length of 512 tokens, truncating longer sequences\n",
    "    model_inputs = tokenizer(examples['article'], max_length=512, truncation=True)\n",
    "    \n",
    "    # Tokenize the 'highlights' text (used as labels) with a maximum length of 150 tokens\n",
    "    labels = tokenizer(examples['highlights'], max_length=150, truncation=True)\n",
    "    \n",
    "    # Set the 'input_ids' from the tokenized highlights as labels for the model\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the tokenization function to the training, validation, and test datasets\n",
    "# The map function applies the tokenization in batches for efficiency\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc82935f-e5fe-404f-8523-ad0447ede2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "\n",
    "# # Load the tokenized datasets\n",
    "# tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "# tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "# tokenized_test_dataset = load_from_disk('tokenized_datasets/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d167e5d0-fad4-4b95-a758-0b7703474d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92fa9149-620b-4528-b7b6-e844ddeb7efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab814086-2a89-43fc-8ae7-b3cb7a6586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate transformers torch datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4d67ef-0c63-4ff2-9d41-6768a3d66d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c5d6e",
   "metadata": {},
   "source": [
    "Adjusting Padding for Tokenized Text Data in T5 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96bc739d-3fed-4698-8ddc-35d41e3c7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958a82a3f2814ea68794eb876129dee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd5c96d7ff04b60800086749599bf0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8cda9bc7ea42e7834b4739c8c4d1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Function to adjust padding\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "# Adjust padding for each dataset\n",
    "max_length_article = 512\n",
    "max_length_summary = 150\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddc15b6",
   "metadata": {},
   "source": [
    "Shuffling, Subsampling, and Padding Adjustments for T5 Model Training on Smaller Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b20329a-0a8d-460b-871c-b6902ccecca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and selecting smaller train dataset...\n",
      "Shuffling and selecting smaller val dataset...\n",
      "Adjusting padding for smaller train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03490827aa04789894c98a7843d62f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting padding for smaller val dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3978fed18404009adedcfeade156cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n",
    "\n",
    "# Function to shuffle and select a subset of the dataset with progress bar\n",
    "def shuffle_and_select(dataset, num_samples, seed=42):\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(indices)\n",
    "    selected_indices = indices[:num_samples]\n",
    "    subset = dataset.select(selected_indices)\n",
    "    return subset\n",
    "\n",
    "# Select a smaller subset of the dataset with progress bar\n",
    "print(\"Shuffling and selecting smaller train dataset...\")\n",
    "small_train_dataset = shuffle_and_select(tokenized_train_dataset, 5000)\n",
    "print(\"Shuffling and selecting smaller val dataset...\")\n",
    "small_val_dataset = shuffle_and_select(tokenized_val_dataset, 1000)\n",
    "\n",
    "# Adjust padding for smaller datasets\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "print(\"Adjusting padding for smaller train dataset...\")\n",
    "small_train_dataset = small_train_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n",
    "print(\"Adjusting padding for smaller val dataset...\")\n",
    "small_val_dataset = small_val_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b332fd32-44e7-4a1f-ae11-3d0b3afaade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['article'], max_length=512, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(examples['highlights'], max_length=150, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cd394",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cba247e-425f-48c0-9775-a7744381c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 5:12:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.321402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_model/t5-small\\\\tokenizer_config.json',\n",
       " 'saved_model/t5-small\\\\special_tokens_map.json',\n",
       " 'saved_model/t5-small\\\\spiece.model',\n",
       " 'saved_model/t5-small\\\\added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Adjust training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # Reduce the number of epochs\n",
    "    per_device_train_batch_size=8,  # Increase the batch size if you have enough GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,  # Use mixed precision training if supported by your hardware\n",
    "    disable_tqdm=False,  # Ensure tqdm progress bar is enabled\n",
    "    report_to=\"none\"  # Ensure no integration with external logging services\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_val_dataset,\n",
    ")\n",
    "\n",
    "# Show progress with tqdm\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('saved_model/t5-small')\n",
    "tokenizer.save_pretrained('saved_model/t5-small')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e71e91a-04b5-43fc-9344-dcd69232c414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: ever noticed how plane seats appear to be getting smaller and smaller with increasing numbers of people taking to the skies some experts are questioning if having such packed out planes is putting passengers at risk they say that the shrinking space on aeroplanes is not only uncomfortable  its putting our health and safety in danger more than squabbling over the arm rest shrinking space on planes putting our health and safety in danger this week a us consumer advisory group set up by the department of transportation said at a public hearing that while the government is happy to set standards for animals flying on planes it doesnt stipulate a minimum amount of space for humans in a world where animals have more rights to space and food than humans said charlie leocha consumer representative on the committee it is time that the dot and faa take a stand for humane treatment of passengers but could crowding on planes lead to more serious issues than fighting for space in the overhead lockers crashing elbows and seat back kicking tests conducted by the faa use planes with a 31 inch pitch a standard which on some airlines has decreased  many economy seats on united airlines have 30 inches of room while some airlines offer as little as 28 inches  cynthia corbertt a human factors researcher with the federal aviation administration that it conducts tests on how quickly passengers can leave a plane but these tests are conducted using planes with 31 inches between each row of seats a standard which on some airlines has decreased reported the detroit news the distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch while most airlines stick to a pitch of 31 inches or above some fall below this while united airlines has 30 inches of space gulf air economy seats have between 29 and 32 inches air asia offers 29 inches and spirit airlines offers just 28 inches british airways has a seat pitch of 31 inches while easyjet has 29 inches thomsons short haul seat pitch is 28 inches and virgin atlantics is 3031\n",
      "Summary: faa use planes with 31 inches between row of seats a standard which on some airlines has decreased many economy seats have 30 inches of room while some airlines offer as little as 28 inches british airways has a seat pitch of 31 inches\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer for inference\n",
    "model = T5ForConditionalGeneration.from_pretrained('saved_model/t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('saved_model/t5-small')\n",
    "\n",
    "# Generate summaries function\n",
    "def generate_summary(text, model, tokenizer, max_length=150, min_length=40, num_beams=4):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, num_beams=num_beams, length_penalty=2.0, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model with a sample text from the test set\n",
    "sample_text = tokenized_test_dataset['article'][0]\n",
    "print(\"Article:\", sample_text)\n",
    "print(\"Summary:\", generate_summary(sample_text, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf88392d-8574-46b4-8b12-c12cff474f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting padding for train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a559269bd6ea44688a06223b04f017a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting padding for val dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013acaef82bc49d185c504b11a21571c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting padding for test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fa666586474a00896b924b1025fe27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n",
    "\n",
    "# Function to adjust padding\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "# Adjust padding for each dataset\n",
    "print(\"Adjusting padding for train dataset...\")\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n",
    "print(\"Adjusting padding for val dataset...\")\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n",
    "print(\"Adjusting padding for test dataset...\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f05d78-a286-4c01-83da-d1cf25e6e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b84401b-4f67-4f2e-8047-8f1e1fc91b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99d94ac-9be4-407e-801c-1115fc7830ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f521213-c081-4b52-b473-5752a9111af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87811796-039b-4ec0-8e38-cd4326c8b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('saved_model/t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('saved_model/t5-small')\n",
    "\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_summary,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"textbox\",\n",
    "    title=\"Text Summarizer\",\n",
    "    description=\"Enter a text to generate its summary using T5 model.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
